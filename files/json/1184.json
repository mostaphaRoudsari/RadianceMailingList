{"refs": ["<B5C4B063-7530-11D9-8104-000A959DDB22@gmx.net>\t<68bb737584aec2065da1f49db1b02eeb@gmail.com>\t<4201F26F.9030809@familyhealth.com.au>\t<Pine.BSF.4.58.0502030812560.40694@emancholl.pair.com>\t<A746D2F0-76BB-11D9-8D94-000A959DDB22@gmx.net>", "<Pine.BSF.4.58.0502040950260.51654@emancholl.pair.com>"], "id": "<42039F05.40103@visarc.com>", "subject": "[Radiance-general] sharing indirect values for parallelprocessing?", "email": "jedev at visarc.com", "body": "\nHi all interested,\n\nCross posting to dev as this is probably a more appropriate space for\nthis conversation.\n\nOK, let's see who I can irritate the most...\n\nAs a refresher, there have been numerous threads on this topic on\nRadiance Dev (in no order other than my searching through my mail):\n\n* Before we give up on lock files...\n* multiprocessor systems, Radiance and you\n* as well as others if you want to delve in to the depths of the pre\nradiance-online mailing list archives\n\nIn general, I recall that there are a couple of directions to go:\n\n* network filesystem locking - such as NFS or Samba, where we are\ndependant on either the locking mechanism actually working (eg\nNFS) or the filesystem (Samba) being installed\n* client/server - probably more hairy from a implementation\nstandpoint as well as from a porting point of view. Although,\nperhaps guaranteeing the best performance for selected os'?\n\nNot to rehash old stuff, but could one of the more knowledgeable\ndevelopers (Greg, Georg, Peter, Carsten...?) give us a refresher on what\nthe options are and perhaps some idea of the time that would be needed\nto implement a workings solution? Locking is a recurring problem. It\nwould be nice to figure a consensus solution (ie what direction to\npursue) and then a strategy for implementation (ie resources, person(s),\nmoney...), so perhaps we as a community could figure out how to move\nthis forward (if as always there is enough interest).\n\nI must admit to having run into this wall on a variety of occasions. NFS\n(v3) on linux is \"supposed\" to lock correctly (sync mode on the\nmount/fstab), as a test there is a test suite from Sun\n(www.connectathon.org) that is supposed to test the nfs server. I\nremember running this test suite in the past and getting positive\nresults on linux. Nevertheless, I have found it extremely difficult to\nget working results with a networked image render (eg rpiece distributed\nover multiple cpu nodes). Either there end up being problems with\nambient values between image cells and/or with locking of the syncfile\nfor distributing image cells to different machines. I even implemented a\nclient/server in perl at one point to try to fight this problem with the\nsyncfile (with partial success as I recall and perhaps more if my time\nwould allow). Not to cause offense... But is it possible that the\nlocking code in Radiance needs to be checked itself?\n\nIn brief follow-up to Lar's comments about openmosix/mosix. As\nunderstand it the msf filesystem, is supposed to implement locking\ncorrectly. There are also other more sophisticated network filesystems\nsuch as GFS (Systina, I think and commercial), OpenGFS and many others.\nHowever these all require separate special install and perhaps\nmodification of the kernel or installation of a modified kernel, and\nthere is serious question as too whether these are portable to other\nos's such as MS version whatever (as the main offender of portability).\n\nNote also that I tried openmosix at one point. One problem that I found\nis that if you start multiple large (eg memory size) jobs on the master\nnode then this can lead to excessive paging and since the master node\ntries to start the jobs at the same time into its own memory space prior\nto migrating them off to other nodes in the cluster. So if your job\nrequires 1 Gig of memory to hold the scene and you want to run 10 jobs\non 5 dual processor nodes with each node having 2 Gig of memory, if you\nstart all the jobs on one node then you are hosed. If you start them on\nindividual nodes, then you should be using a different clustering\nsolution since this completely negates the value of the migration\nalgorithms in openmosix. Now it has been a while since I used OpenMosix,\nso perhaps things are different...\n\nNote also that named pipes do not work (at least back in mid 2003, you\ncan see my brief inquiry to the openmosix list and Moshe Bar's even\nbriefer reply back in April of 2003) on OpenMosix. So if you want to do\nmemory sharing on multiprocessor nodes you have to roll your own batch\njob distributor.\n\n-Jack de Valpine\n\nGeorg Mischler wrote:\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: http://radiance-online.org/pipermail/radiance-general/attachments/20050204/11a788c3/attachment.htm\n", "isquestion": false, "replyTo": "<Pine.BSF.4.58.0502040950260.51654@emancholl.pair.com>", "tags": [], "sender": "Jack de Valpine", "datetime": "Fri Feb  4 17:13:34 2005"}