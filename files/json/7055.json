{"refs": [], "id": "", "subject": "[Radiance-general] \"Broken pipe\" message from rpiece onmulti-core Linux system", "email": "Jack de Valpine", "body": "Message-ID: <CBAB1D93.F5B8%robert.guglielmetti@nrel.gov>\n\nHey Andy,\n\nThanks as always for sharing this. I tried to do something similar a couple years ago, and Greg even pointed me to the bit of code in rpiece that does the tiling, but I could never get it to work for me. This is great.\n\n- Rob\n\n\nOn 4/11/12 11:04 AM, \"Andy McNeil\" <amcneil at lbl.gov<mailto:amcneil at lbl.gov>> wrote:\n\nHi Randolph,\n\nFor what it's worth I don't use rpiece when I render on the cluster.  I have a script that takes divides takes a view file, tile number and number of rows an columns and will render the assigned tile number (run_render.csh).   In the job submit script I distribute these tile rendering tasks to multiple cores on multiple nodes.  I can't use the ambient cache with this method, but i typically use rtcontrib so I would be able to use it regardless.  There is also the problem that some processors sit idle after they've finished their tile while other processes are running, but I don't worry about it because computing time on lawrencium is cheap and available.\n\nSnippets from my scripts are below.\n\nAndy\n\n\n\n### job_submitt.bsh #####\n\n#!/bin/bash\n#    specify the queue: lr_debug, lr_batch\n#PBS -q lr_batch\n#PBS -A ac_rad71t\n#PBS -l nodes=16:ppn=8:lr1\n#PBS -l walltime=24:00:00\n#PBS -m be\n#PBS -M amcneil at lbl.gov<mailto:amcneil at lbl.gov>\n#PBS -e run_v4a.err\n#PBS -o run_v4a.out\n\n#   change to working directory & run the program\ncd ~/models/wwr60\n\nfor i in {0..127}; do\npbsdsh -n $i $PBS_O_WORKDIR/run_render.csh views/v4a.vf $(printf \"%03d\" $i) 8 16 &\ndone\n\nwait\n\n\n\n\n### run_render.csh ######\n#! /bin/csh\n\ncd $PBS_O_WORKDIR\nset path=($path ~/applications/Radiance/bin/ )\n\nset oxres = 512\nset oyres = 512\n\nset view = $argv[1]\nset thispiece = $argv[2]\nset numcols = $argv[3]\nset numrows = $argv[4]\nset numpieces = `ev \"$numcols * $numrows\"`\n\nset pxres = `vwrays -vf $view -x $oxres -y $oyres -d | awk '{print int($2/'$numcols'+.5)}'`\nset pyres = `vwrays -vf $view -x $oxres -y $oyres -d | awk '{print int($4/'$numrows'+.5)}'`\n\nset vtype = `awk '{for(i=1;i<NF;i++) if(match($i,\"-vt\")==1) split($i,vt,\"\")} END { print vt[4] }' $view`\nset vshift = `ev \"$thispiece - $numcols * floor( $thispiece / $numcols) - $numcols / 2 + .5\"`\nset vlift = `ev \"floor( $thispiece / $numcols ) - $numrows / 2 + .5\"`\n\nif ($vtype == \"v\") then\nset vhoriz = `awk 'BEGIN{PI=3.14159265} \\\n{for(i=1;i<NF;i++) if($i==\"-vh\") vh=$(i+1)*PI/180 } \\\nEND{print atan2(sin(vh/2)/'$numcols',cos(vh/2))*180/PI*2}' $view`\nset vvert = `awk 'BEGIN{PI=3.14159265} \\\n{for(i=1;i<NF;i++) if($i==\"-vv\") vv=$(i+1)*PI/180 } \\\nEND{print atan2(sin(vv/2)/'$numrows',cos(vv/2))*180/PI*2}' $view`\nendif\n\nvwrays -ff -vf $view -vv $vvert -vh $vhoriz -vs $vshift -vl $vlift -x $pxres -y $pyres \\\n| rtcontrib -n 1 `vwrays -vf $view -vv $vvert -vh $vhoriz -vs $vshift -vl $vlift -x $pxres -y $pyres -d` \\\n-ffc -fo \\\n-o binpics/wwr60/${view:t:r}/${view:t:r}_wwr60_%s_%04d_${thispiece}.hdr \\\n-f klems_horiz.cal -bn Nkbins \\\n-b 'kbin(0,1,0,0,0,1)' -m GlDay -b 'kbin(0,1,0,0,0,1)' -m GlView \\\n-w -ab 6 -ad 6000 -lw 1e-7 -ds .07 -dc 1 oct/vmx.oct\n\n\n\n\n\n\n\nOn Apr 11, 2012, at 5:54 AM, Jack de Valpine wrote:\n\nHi Randolph,\n\nAll I have is Linux. Not sure what kernels at this point. But I have noticed this over multiple kernels and distributions. Although I have not run anything on the most recent kernels.\n\nI know that one thing I did was to disable the fork and wait functionality in rpiece to wait for a job to finish. I do not recall though if this was related to this problem, nfs locking, or running on a cluster with job distribution queuing....? Sorry I do not remember more right now.\n\nJust thinking out loud here, but if you are running on a cluster then could network latency also be an issue?\n\nHere is my suspicion/theory, which I have not been able to test. I think that somehow there is a race condition in the way jobs get forked off and status of pieces gets recorded in the syncfile...\n\nFor testing/debugging purposes, a few things to look at compare might be:\n\n*   big scene - slow load time\n*   small scene - fast load time\n*   \"fast\" parameters - small image size with lots of divisions\n*   \"slow\" parameters - small image size with lots of divisions\n\nOn my cluster, I ended up setting up things so that any initial small image run for building the ambient cache would actually just run as a single rpict process and then large images would get distributed across nodes/cores.\n\nAs an aside, perhaps Rob G. has some thoughts on Radiance/Clusters as I think they have a large one also. What is the cluster set up at LBNL? I believe that at one point they were using a provisioning system called Warewulf which has now evolved to Perceus. I have the former setup and have not gotten around to the latter. LBNL may also be using a job queuing system called Slurm which they developed (or maybe that was at LLNL)?\n\nHopefully this is not leading you off on the wrong track though. Probably would be useful to figure out if the problem is indeed rpiece related or something else entirely.\n\n-Jack\n\n--\n# Jack de Valpine\n# president\n#\n# visarc incorporated\n# http://www.visarc.com<http://www.visarc.com/>\n#\n# channeling technology for superior design and construction\n\nOn 4/11/2012 1:27 AM, Randolph M. Fritz wrote:\n\nThanks Jack, Greg.\n\n\nJack, what kernel were you using?  Was it also Linux?\n\n\nGreg, I was using rad, so those delays are already in there, alas.  I wonder if there is some subtle difference between the Mac OS Mach kernel and the Linux kernel that's causing the problem, or if it occurs on all platforms, just more frequently in the very fast cluster nodes.\n\n\nOr, it could be an NFS locking problem, bah.\n\n\nIf I find time, maybe I can dig into it some more.  Right now, I may just finesse it by running multiple *different* simulations on the same cluster node.\n\n\nRandolph\n\n\nOn 2012-04-09 21:52:47 +0000, Greg Ward said:\n\n\nIf it is a startup issue as Jack suggests, you might try inserting a few seconds of delay between the spawning of each new rpiece process using \"sleep 5\" or similar.  This allows time for the sync file to be updated without contention between processes.  This is what I do in rad with the -N option.  I actually wait 10 seconds between each new rpiece process.\n\n\nThis isn't to say that I understand the source of your error, which still puzzles me.\n\n\n-Greg\n\n\n\n\nHey Randolph,\n\n\nI have run into this before. Unfortunately I have had limited success in tracking down the issue and also have not really looked at it for some time. If I recall correctly, a couple of things that I have noticed:\n\n*   possible problem if a piece finishes before the first set of pieces are parcelled out out by rpiece - so if it 8 pieces are being distributed at startup and piece 2 (for example) finishes before one of pieces 1, 3, 4, 5, 6, 7, 8 has even been processed by rpiece or while rpiece is still forking off the initial jobs.\n\nSorry I cannot offer more, I have spent some time in the code on this one and it is not for the faint of heart to say the least.\n\n-Jack\n\n--\n\n# Jack de Valpine\n\n# president\n\n#\n\n# visarc incorporated\n\n# http://www.visarc.com<http://www.visarc.com/>\n\n#\n\n# channeling technology for superior design and construction\n\n\nOn 4/9/2012 3:29 PM, Randolph M. Fritz wrote:\n\nThis problem is back for a sequel, and it would really help my work if I could get it going.\n\n\nIt's been a few months since I last asked about this.  Has anyone else experienced this in a Linux environment?  Anyone have any ideas what to do about it or how to debug it?\n\n\n/proc/version reports:\n\nLinux version 2.6.18-274.18.1.el5 (mockbuild-t2f/um9L7dhDWr0U+X5jBOG/Ez6ZCGd0 at public.gmane.orgorg<mailto:mockbuild at builder10.centos.org>) (gcc version 4.1.2 20080704 (Red Hat 4.1.2-51)) #1 SMP Thu Feb 9 12:45:44 EST 2012\n\n\nRandolph\n\n\nOn 2011-07-08 01:13:01 +0000, Randolph M. Fritz said:\n\n\nOn 2011-07-07 16:54:06 -0700, Greg Ward said:\n\n\nHi Randolph,\n\n\nThis shouldn't happen, unless one of the rpict processes died\n\nunexpectedly.  Even then, I would expect some other kind of error to be\n\nreported as well.\n\n\n-Greg\n\n\nThanks, Greg.  I think that's what happenned; in fact seven of the\n\neight died in two cases.  Wierdly, the third succeeded.  If I run it as\n\na single-processor job, it works.  Here's a piece of the log:\n\n\nrpiece -F bl_blinds_rpsync.txt -PP pfLF5M90 -vtv -vp 60.0 -2.0 66.0 -vd\n\n12.0 0.0 0.0 -vu 0 0 1 -vh 60 -x 1024 -y 1024 -dp 512 -ar 42 -ms 3.6\n\n-ds .3 -dt .1 -dc .5 -dr 1 -ss 1 -st .1 -af bl.amb -aa .1 -ad 1536 -as\n\n392 -av 10 10 10 -lr 8 -lw 1e-4 -ps 6 -pt .08 -o bl_blinds.unf bl.oct\n\n\nrpict: warning - no output produced\n\n\nrpict: system - write error in io_process: Broken pipe\n\nrpict: 0 rays, 0.00% after 0.000u 0.000s 0.001r hours on n0065.lr1\n\nrad: error rendering view blinds\n\n\n\n_______________________________________________\n\nRadiance-general mailing list\n\nRadiance-general at radiance-online.org<mailto:Radiance-general at radiance-online.org>\n\nhttp://www.radiance-online.org/mailman/listinfo/radiance-general\n\n_______________________________________________\n\nRadiance-general mailing list\n\nRadiance-general at radiance-online.org<mailto:Radiance-general at radiance-online.org>\n\nhttp://www.radiance-online.org/mailman/listinfo/radiance-general\n\n\n\n--\n\nRandolph M. Fritz\n\n\n\n_______________________________________________\nRadiance-general mailing list\nRadiance-general at radiance-online.org<mailto:Radiance-general at radiance-online.org>http://www.radiance-online.org/mailman/listinfo/radiance-general\n\n_______________________________________________\nRadiance-general mailing list\nRadiance-general at radiance-online.org<mailto:Radiance-general at radiance-online.org>\nhttp://www.radiance-online.org/mailman/listinfo/radiance-general\n\n\n\n", "isquestion": false, "replyTo": "<995E2963-96E2-48C4-BC7E-C89FD5EF32B7@lbl.gov>", "tags": [], "sender": "jedev at visarc.com", "datetime": "April 9, 2012 1:46:03 PM PDT"}