{"body": "Randolph Fritz wrote:\n\n\n> It's more the network and processor overhead of TCP, though the\n> limited number of file descriptors was once a problem.\n\n\nI understand that TCP has a much higher overhead than UDP. But is\nthis really significant compared to the processing that we\nalready do with Radiance? Five times very little may still not\nbe much, after all...\n\n\n\n\n> I don't think\n> a single app could ever really manage 1024 simultaneous TCP\n> connections on most systems, except for the big IBM servers.\n\n\nThe sockets as such are not a problem, but the polling (even when\ndone by the OS through select()) will become a bit less efficient\neventually. This article talks about an example implemented in\nPython on PC hardware:\n  http://www.nightmare.com/medusa/async_tweaks.html\n\n\n\n\n> I think you may be underestimating the problem of recovery from\n> dropped connections, downed processors, and so on.\n\n\nWe're not handling life support systems here, are we? ;)\n\n\nThe server doesn't really care about individual client\nconnections. Even if the server dies and restarts, it would be\npossible for the clients to try to reconnect a few times within\nreasonable intervals.\n\n\n\n\n> If we can assume a shared file system, there's a simpler way.\n>\n> Have one ambient server app that writes the shared file.  The server\n> receives data via UDP from the various renderers, and adds it to the\n> file.  (Is conflicting data an issue?  How best for the server to\n> resolve it?)  Every time the renderers read the file they check to\n> make sure the items they sent are in it; if they're not, they resend\n> them.\n\n\nInteresting idea. Now you see why I want to implement the server\nin Python at first, so we can play with various concepts and see\nwhich behaves best under load on all platforms.\n\n\nConflicting data are not a problem, at least not one that is\nspecific to this setup. Those are also written to the same file\nby the traditional sharing method.\n\n\nMy initial reflex was to use the same (TCP) channel for both\ndirections, but you're right that this isn't really necessary.\nThe simulation jobs need file system access to lots of common\ndata anyway, so there's no reason why the ambient server should\nbe excluded from that. That way, the reading channel is secured\nagainst packet loss by NFS.\n\n\nThis leaves us with the choice of UDP or TCP for collecting the\ndata. The final solution will have to balance the overhead of TCP\nagainst the inherent unreliability of UDP.\n\n\nIf an UDP packet with ambient data for storage is dropped, the\nconsequence is simply that those spefic values will be missing in\nthe file. Another process that later checks for them will not find\nanything, and therefore has to do the same calculation again, or\nat least a very similar one. This means that any dropped packets\nwill cause the overall computation time to go up, whether they\nare caused by network congestion or by the server temporarily\ngoing offline. We'll have to test what happens when increasing\nnumbers of packets get dropped (1%, 5%, etc.), but I don't expect\nany problems besides the wasted time (Greg?).\n\n\nIf we use TCP, then we know that no packets are lost, or rather\nwe'll know when it happens. The clients will first block if the\nserver doesn't answer, and after a while the connection may be\ndeclared dead. This would give us some primitive built-in\nmonitoring right at the place where this information is needed,\nwithout introducing yet another process.\n\n\nI don't want to make any predictions at the moment about which\nmethod will me more efficient in practise. It won't take much to\nimplement both of them, and then we'll have to ask people to run\nlots of tests under heavy load.\n\n\n\n\n>   - I recommend that renderers keep a timer, and if the server does\n>     not see ambient file updates in some operator-determined period of\n>     time, raise an alarm.  (The operator-determined time because it\n>     depends on network and filesystem latency.)\n\n\nI'm not sure what problem you're trying to solve here. Ambient\ndata updates can get rather sparse towards the end of a\nsimulation anyway, so that their frequency isn't really a good\nindicator of anything.\n\n\n\n\n>   - I believe the ambient updater itself could be best organized as a\n>     monitor and an updater process;\n\n\nI don't think that job management concepts are within the\nfunctional scope of core Radiance. Stuff like that can be easily\nadded from other sources.\n\n\n\n\n>   - The updater can publish its UDP port and a 64-bit random magic\n\n\nLet's worry about security once the functionality as such works.\nIt will be trivial to have the server only accept packets from\nthe local IP range, and you should have a firewall in front of\nyour network in any case.\n\n\n\n\n-schorsch\n\n\n\n\nGeorg Mischler  --  simulations developer  --  schorsch at schorsch com\n+schorsch.com+  --  lighting design tools  --  http://www.schorsch.com/\n___\n<sup>Automatically generated content from [radiance mailing-list](https://radiance-online.org/pipermail/radiance-dev/2003-January/000036.html).</sup>", "attachments": [], "created_by_name": "Georg Mischler", "created_at": "January 31, 2003 at 08:03AM", "created_by": "Georg_Mischler", "parent_id": "radiance-dev_000016", "id": "radiance-dev_000036"}