{"body": "Schorsch wrote:\n> You almost convinced me with this one, until I checked the\n> open(2) man page on my Linux box:\n>\n>   O_EXCL is broken on\n>   NFS  file  systems,  programs  which  rely on it for performing\n>   locking tasks will contain a race condition.  The solution  for\n>   performing  atomic file locking using a lockfile is to create a\n>   unique file on the same fs (e.g.,  incorporating  hostname  and\n>   pid),  use  link(2)  to  make a link to the lockfile. If link()\n>   returns 0, the lock is successful.  Otherwise, use  stat(2)  on\n>   the  unique file to check if its link count has increased to 2,\n>   in which case the lock is also successful.\n>\n> I don't know if this is specific to Linux, or if is just the only\n> system where the documentation spells out the problem. The method\n> with the hard link is what Mailman uses, btw., making reference to\n> the above paragraph of the linux man page in the code.\n\n\nHmmm...  Is Everything broken on Linux, or does it just seem that way?  \nThe NFS lock manager doesn't work, O_EXCL doesn't work, what are they \ngoing to throw in our path, next?  I found no such warning on my OS X \nmachine, though as you say, the bug could be there and just not be \ndocumented...  Anyway, it's very nice that they included a workaround, \nwhich isn't too awful, though I don't get the part about the link() \ncall failing but still succeeding.  Very strange.\n\n\n> Relying on the ctime of the file makes us dependent on the system\n> clocks being exactly snychronized. It is not uncommon for\n> machines in the same network to by out of sync by several\n> minutes. Maybe we just have to watch the file for at least a\n> minute to decide it's expired.\n\n\nI thought about this, and that's why I think it's best for the process \nto simply keep track of when the lock file says it was created, and if \nthe date hasn't changed between checks that are a few minutes apart by \nthe local clock, it's safe to assume that the process died during an \nupdate.\n\n\nI think your other scenarios are getting a bit far-fetched, especially \nif we follow the strategy I recommended of removing the lock file in \none process, but not assuming then that we can claim the lock ourselves \n-- just continuing to render until the next normal checkpoint.  Then, \nyour second-tier race condition would require the simultaneous lock \nremoval by two processes coincident with the lock-assertion of a third \nprocess.  Unless we are really stupid about how often we check the lock \nfile, I don't think this scenario will play out in any of our \nlifetimes.  The chance of a process dying in the middle of an update \nalone is probably small, and you're multiplying this by the probability \nof a three-way tie.  At some point, we have to say, \"that's good \nenough.\"  I don't want to implement a second lock file to remove the \nfirst one in the event of a process dying mid-write.\n\n\n-Greg\n___\n<sup>Automatically generated content from [radiance mailing-list](https://radiance-online.org/pipermail/radiance-dev/2003-February/000044.html).</sup>", "attachments": [], "created_by_name": "Greg Ward", "created_at": "February 02, 2003 at 05:54PM", "created_by": "Greg_Ward", "parent_id": "radiance-dev_000042", "id": "radiance-dev_000044"}