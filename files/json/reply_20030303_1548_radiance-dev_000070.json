{"body": "--0-1890075824-1046735293=:9032\nContent-Type: text/plain; charset=us-ascii\n\n\n\n\nGreg,\nWhat about a heirarchical boundary representation?  One challenge of optimization is to minimize repeated vertex points, and another is to optimize the ray intersection calculation.  Perhaps this idea would address both?\nIf the mesh at the top level is idealized as an n-sided polygon surrounding the entire collection of polygons, then as the ray \"gets closer,\" the smaller \"rings\" of polygons are further resolved into smaller and smaller \"idealized\" n-sided polygons, until you are only left with one \"loop\" of triangles or polygons.  I'm imagining that a clever data structure could be developed where each successive \"loop\" of triangles can reference the vertex lists (by index) one level \"up\" and one level \"down\" the tree, thereby eliminating duplication.  The mesh would have to be pre-processed to order the vertexes accordingly, and would have to assume that all polygons are edge coherent.\nMake any sense?  Or am I just not grocking the problem?\n-Chas\n Greg Ward <gward@lmi.net> wrote:schorsch writes:\n\n\n> Isn't that rather a side issue of the whole feature anyway?\n> It might be worth to just implement it in the straightforward\n> way, and only then to look into further optimizations.\n> I think that keeping the code reasonably simple should also be a\n> design goal, that should only be thrown out for significant gains\n> in either speed or memory use. Neither of those seems very\n> obvious for the moment in the context of face grouping. Investing\n> a lot of thinking and coding into another 20 or 30% after we\n> already raised the bar by more than an order of magnitude doesn't\n> sound like a priority task to me...\n\n\nActually, memory use is the central issue in my mind. (Don't take that \ntoo literally.) If memory weren't an issue, there'd be no real reason \nto implement a mesh primitive. The problems that Peter A-B mentions \nregarding certain material types not working properly with interpolated \nnormals is an embarrassment. In fact, I don't know why this bug stood \nfor so long -- I had a look at the relevant code, and the solution was \nquite simple. It should be fixed in the upcoming release, due out very \nshortly!\n\n\nMy main purpose for implementing meshes is to provide support for very \nlarge, complicated geometries, as one might obtain from a laser range \nscanner, for example. Scanned meshes are required for many \narcheological reconstructions and other scenes captured from real \ngeometry, and each mesh may contain hundreds of thousands or even \nmillions of triangles. Memory and its influence on virtual memory \nperformance are the main rendering challenges. If I give up on local \nvertex references in faces, which I may have to unless I can resolve \nsome sticky problems, the memory use doesn't go up by 30% -- it \ntriples! Pointers take the majority of space in a standard mesh \nrepresentation, and I would like to minimize these expenses any way I \ncan.\n\n\n-Greg\n\n\n_______________________________________________\nRadiance-dev mailing list\nRadiance-dev@radiance-online.org\nhttp://www.radiance-online.org/mailman/listinfo/radiance-dev\n--0-1890075824-1046735293=:9032\nContent-Type: text/html; charset=us-ascii\n\n\n<P>Greg,\n<P>What about a heirarchical boundary representation?&nbsp; One challenge of optimization is to minimize repeated vertex points, and another is to optimize the ray intersection calculation.&nbsp; Perhaps this idea would address both?\n<P>If the mesh at the top level is idealized as an n-sided polygon surrounding the entire collection of polygons,&nbsp;then as the ray \"gets closer,\" the smaller \"rings\" of polygons are further resolved into smaller and smaller \"idealized\" n-sided polygons, until you are only left with one \"loop\" of triangles or polygons.&nbsp; I'm imagining that a clever data structure could be developed where each successive \"loop\" of triangles can reference the vertex lists (by index) one level \"up\" and one level \"down\" the tree, thereby eliminating duplication.&nbsp; The mesh would have to be pre-processed to order the vertexes accordingly, and would have to assume that all polygons are edge coherent.\n<P>Make any sense?&nbsp; Or am I just not grocking the problem?\n<P>-Chas\n<P>&nbsp;<B><I>Greg Ward &lt;gward@lmi.net&gt;</I></B> wrote:\n<BLOCKQUOTE style=\"PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #1010ff 2px solid\">schorsch writes:<BR><BR>&gt; Isn't that rather a side issue of the whole feature anyway?<BR>&gt; It might be worth to just implement it in the straightforward<BR>&gt; way, and only then to look into further optimizations.<BR>&gt; I think that keeping the code reasonably simple should also be a<BR>&gt; design goal, that should only be thrown out for significant gains<BR>&gt; in either speed or memory use. Neither of those seems very<BR>&gt; obvious for the moment in the context of face grouping. Investing<BR>&gt; a lot of thinking and coding into another 20 or 30% after we<BR>&gt; already raised the bar by more than an order of magnitude doesn't<BR>&gt; sound like a priority task to me...<BR><BR>Actually, memory use is the central issue in my mind. (Don't take that <BR>too literally.) If memory weren't an issue, there'd be no real reason <BR>to implement a mesh primitive. The problems that Peter A-B mentions <BR>regarding certain material types not working properly with interpolated <BR>normals is an embarrassment. In fact, I don't know why this bug stood <BR>for so long -- I had a look at the relevant code, and the solution was <BR>quite simple. It should be fixed in the upcoming release, due out very <BR>shortly!<BR><BR>My main purpose for implementing meshes is to provide support for very <BR>large, complicated geometries, as one might obtain from a laser range <BR>scanner, for example. Scanned meshes are required for many <BR>archeological reconstructions and other scenes captured from real <BR>geometry, and each mesh may contain hundreds of thousands or even <BR>millions of triangles. Memory and its influence on virtual memory <BR>performance are the main rendering challenges. If I give up on local <BR>vertex references in faces, which I may have to unless I can resolve <BR>some sticky problems, the memory use doesn't go up by 30% -- it <BR>triples! Pointers take the majority of space in a standard mesh <BR>representation, and I would like to minimize these expenses any way I <BR>can.<BR><BR>-Greg<BR><BR>_______________________________________________<BR>Radiance-dev mailing list<BR>Radiance-dev@radiance-online.org<BR>http://www.radiance-online.org/mailman/listinfo/radiance-dev</BLOCKQUOTE>\n--0-1890075824-1046735293=:9032--\n___\n<sup>Automatically generated content from [radiance mailing-list](https://radiance-online.org/pipermail/radiance-dev/2003-March/000070.html).</sup>", "attachments": [], "created_by_name": "Charles Ehrlich", "created_at": "March 03, 2003 at 03:48PM", "created_by": "Charles_Ehrlich", "parent_id": "radiance-dev_000056", "id": "radiance-dev_000070"}