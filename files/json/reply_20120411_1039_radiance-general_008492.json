{"body": "Hey Andy,\n\n\nThanks as always for sharing this. I tried to do something similar a couple years ago, and Greg even pointed me to the bit of code in rpiece that does the tiling, but I could never get it to work for me. This is great.\n\n\n- Rob\n\n\n\n\nOn 4/11/12 11:04 AM, \"Andy McNeil\" <amcneil at lbl.gov<mailto:amcneil at lbl.gov>> wrote:\n\n\nHi Randolph,\n\n\nFor what it's worth I don't use rpiece when I render on the cluster.  I have a script that takes divides takes a view file, tile number and number of rows an columns and will render the assigned tile number (run_render.csh).   In the job submit script I distribute these tile rendering tasks to multiple cores on multiple nodes.  I can't use the ambient cache with this method, but i typically use rtcontrib so I would be able to use it regardless.  There is also the problem that some processors sit idle after they've finished their tile while other processes are running, but I don't worry about it because computing time on lawrencium is cheap and available.\n\n\nSnippets from my scripts are below.\n\n\nAndy\n\n\n\n\n\n\n### job_submitt.bsh #####\n\n\n #!/bin/bash\n #    specify the queue: lr_debug, lr_batch\n #PBS -q lr_batch\n #PBS -A ac_rad71t\n #PBS -l nodes=16:ppn=8:lr1\n #PBS -l walltime=24:00:00\n #PBS -m be\n #PBS -M amcneil at lbl.gov<mailto:amcneil at lbl.gov>\n #PBS -e run_v4a.err\n #PBS -o run_v4a.out\n\n\n#   change to working directory & run the program\ncd ~/models/wwr60\n\n\nfor i in {0..127}; do\npbsdsh -n $i $PBS_O_WORKDIR/run_render.csh views/v4a.vf $(printf \"%03d\" $i) 8 16 &\ndone\n\n\nwait\n\n\n\n\n\n\n\n\n### run_render.csh ######\n#! /bin/csh\n\n\ncd $PBS_O_WORKDIR\nset path=($path ~/applications/Radiance/bin/ )\n\n\nset oxres = 512\nset oyres = 512\n\n\nset view = $argv[1]\nset thispiece = $argv[2]\nset numcols = $argv[3]\nset numrows = $argv[4]\nset numpieces = `ev \"$numcols * $numrows\"`\n\n\nset pxres = `vwrays -vf $view -x $oxres -y $oyres -d | awk '{print int($2/'$numcols'+.5)}'`\nset pyres = `vwrays -vf $view -x $oxres -y $oyres -d | awk '{print int($4/'$numrows'+.5)}'`\n\n\nset vtype = `awk '{for(i=1;i<NF;i++) if(match($i,\"-vt\")==1) split($i,vt,\"\")} END { print vt[4] }' $view`\nset vshift = `ev \"$thispiece - $numcols * floor( $thispiece / $numcols) - $numcols / 2 + .5\"`\nset vlift = `ev \"floor( $thispiece / $numcols ) - $numrows / 2 + .5\"`\n\n\nif ($vtype == \"v\") then\nset vhoriz = `awk 'BEGIN{PI=3.14159265} \\\n{for(i=1;i<NF;i++) if($i==\"-vh\") vh=$(i+1)*PI/180 } \\\nEND{print atan2(sin(vh/2)/'$numcols',cos(vh/2))*180/PI*2}' $view`\nset vvert = `awk 'BEGIN{PI=3.14159265} \\\n{for(i=1;i<NF;i++) if($i==\"-vv\") vv=$(i+1)*PI/180 } \\\nEND{print atan2(sin(vv/2)/'$numrows',cos(vv/2))*180/PI*2}' $view`\nendif\n\n\nvwrays -ff -vf $view -vv $vvert -vh $vhoriz -vs $vshift -vl $vlift -x $pxres -y $pyres \\\n| rtcontrib -n 1 `vwrays -vf $view -vv $vvert -vh $vhoriz -vs $vshift -vl $vlift -x $pxres -y $pyres -d` \\\n-ffc -fo \\\n-o binpics/wwr60/${view:t:r}/${view:t:r}_wwr60_%s_%04d_${thispiece}.hdr \\\n-f klems_horiz.cal -bn Nkbins \\\n-b 'kbin(0,1,0,0,0,1)' -m GlDay -b 'kbin(0,1,0,0,0,1)' -m GlView \\\n-w -ab 6 -ad 6000 -lw 1e-7 -ds .07 -dc 1 oct/vmx.oct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Apr 11, 2012, at 5:54 AM, Jack de Valpine wrote:\n\n\nHi Randolph,\n\n\nAll I have is Linux. Not sure what kernels at this point. But I have noticed this over multiple kernels and distributions. Although I have not run anything on the most recent kernels.\n\n\nI know that one thing I did was to disable the fork and wait functionality in rpiece to wait for a job to finish. I do not recall though if this was related to this problem, nfs locking, or running on a cluster with job distribution queuing....? Sorry I do not remember more right now.\n\n\nJust thinking out loud here, but if you are running on a cluster then could network latency also be an issue?\n\n\nHere is my suspicion/theory, which I have not been able to test. I think that somehow there is a race condition in the way jobs get forked off and status of pieces gets recorded in the syncfile...\n\n\nFor testing/debugging purposes, a few things to look at compare might be:\n\n\n *   big scene - slow load time\n *   small scene - fast load time\n *   \"fast\" parameters - small image size with lots of divisions\n *   \"slow\" parameters - small image size with lots of divisions\n\n\nOn my cluster, I ended up setting up things so that any initial small image run for building the ambient cache would actually just run as a single rpict process and then large images would get distributed across nodes/cores.\n\n\nAs an aside, perhaps Rob G. has some thoughts on Radiance/Clusters as I think they have a large one also. What is the cluster set up at LBNL? I believe that at one point they were using a provisioning system called Warewulf which has now evolved to Perceus. I have the former setup and have not gotten around to the latter. LBNL may also be using a job queuing system called Slurm which they developed (or maybe that was at LLNL)?\n\n\nHopefully this is not leading you off on the wrong track though. Probably would be useful to figure out if the problem is indeed rpiece related or something else entirely.\n\n\n-Jack\n\n\n\n\n# Jack de Valpine\n# president\n#\n# visarc incorporated\n# http://www.visarc.com<http://www.visarc.com/>\n#\n# channeling technology for superior design and construction\n\n\nOn 4/11/2012 1:27 AM, Randolph M. Fritz wrote:\n\n\nThanks Jack, Greg.\n\n\n\n\nJack, what kernel were you using?  Was it also Linux?\n\n\n\n\nGreg, I was using rad, so those delays are already in there, alas.  I wonder if there is some subtle difference between the Mac OS Mach kernel and the Linux kernel that's causing the problem, or if it occurs on all platforms, just more frequently in the very fast cluster nodes.\n\n\n\n\nOr, it could be an NFS locking problem, bah.\n\n\n\n\nIf I find time, maybe I can dig into it some more.  Right now, I may just finesse it by running multiple *different* simulations on the same cluster node.\n\n\n\n\nRandolph\n\n\n\n\nOn 2012-04-09 21:52:47 +0000, Greg Ward said:\n\n\n\n\nIf it is a startup issue as Jack suggests, you might try inserting a few seconds of delay between the spawning of each new rpiece process using \"sleep 5\" or similar.  This allows time for the sync file to be updated without contention between processes.  This is what I do in rad with the -N option.  I actually wait 10 seconds between each new rpiece process.\n\n\n\n\nThis isn't to say that I understand the source of your error, which still puzzles me.\n\n\n\n\n-Greg\n___\n<sup>Automatically generated content from [radiance mailing-list](https://radiance-online.org/pipermail/radiance-general/2012-April/008492.html).</sup>", "attachments": [], "created_by_name": "Rob Guglielmetti", "created_at": "April 11, 2012 at 10:39AM", "created_by": "Rob_Guglielmetti", "parent_id": "radiance-general_008486", "id": "radiance-general_008492"}