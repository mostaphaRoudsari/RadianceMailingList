{"topic": "radiance + openmosix = :)", "category": "radiance-general", "attachments": [], "created_by_name": "Francesco Anselmo", "created_at": "August 28, 2004 at 12:43AM", "body": "Hi all,\n\n\nI had some time to do a few experiments with\nopenmosix and radiance during the past weeks.\nAnd, luckily, I've been successful, or at least\nnow I know a little bit more about both systems ...\n\n\nMaybe somebody can be interested ...\n\n\nProbably many of you already know what openmosix is\n(and of course _all_ of you should know about radiance,\neven if day by day we discover something new about it ...):\nas far as I've understood, openmosix is a linux kernel \nextension that provides single image clustering \ncapabilities to a network of linux boxes.\nIn other words, instead of having a machine with a single\nprocessor, it is possible to see as many processors\nas all your networked linux boxes have.\n\n\nThe main advantages of the openmosix approach to do parallel\ncomputation with radiance are:\n\n\n1. no need to use nfs (it can really be painful to choose\n   the right version and to set up the lockd daemon with\n   linux)\n2. no need to login to different machines to launch the rpiece\n   processes (no rsh or ssh or telnet or whatever needed)\n3. it works with rholo -n x, when x>1\n\n\nWhat you need: a few computers laying around, a linux distro\n(I prefer and suggest debian or knoppix/clusterknppix, maybe \ngentoo can also be practical), a newtork switch + cables, \ntime, patience.\n\n\nAt work, I've been able to put over and under my desk (kidnapped) \n1, 2, 3, ..., 8 pc's, some old, some new (from 433Mhz PIII to 2.2Ghz PIV), \nand I started doing some tests in my very little spare time. \nIt is a little bit hot now, and I get a headache after 5 minutes,\nbut ...\n\n\nFirst I installed gnu/debian on each of them (well, I installed\nit on one and then cloned it with rsync), and then recompiled\nthe linux kernel (2.4.26) with the unofficial openmosix patch\nfrom http://openmosix.snarc.org/download: I used the unofficial\none because it is more updated, and I needed to have at least \nkernel 2.4.26 for a couple of hardware devices on my laptop to work:\nit is very important for all the machines in the network to\nhave the same kernel and openmosix versions, otherwise they\nwon't communicate. There are no openmosix patches for 2.6.x kernels, \nyet.\n\n\nAfter installing the kernel and compiling and installing radiance\n(I used the optimisations for the slowest processors in the cluster,\nPIII), I verified that everything was working: the new openmosix releases\ninclude an autodiscovery daemon that takes care of automaticaly\nadding new nodes to the cluster when they are available, and also use a \nspecial load balancing algorithm to automatically migrate the processes to \nthe faster nodes. \nDebian has two openmosix packages, one for the patch (the patch is not very \nupdated, 2.4.24, but also installs the daemon init scripts) and another for \nthe userspace programs, to monitor the cluster and manage the migration\nprocess. There also is an additional graphical interface, openmosixview,\nquite helpful.\n\n\nThe first test I did was with rholo, and it worked pretty well with the\nnetwork of slowest computers: when adding the faster ones, some processes\nwere migrating to the fastest machines, other were spending a lot of time\nin deciding what to do, adding overhead to the system and slowing down\nthe simulation. With rpiece it was even worse, and I was starting losing\nmy faith ...\n\n\nAs you may have noticed, my system is very asymmetrical, and this was\nthe cause of many failures.\n\n\nAfter that, I've tried with the fastest ones (2.0 and 2.2GHz PIV), and\nafter some failures I realized that I had to launch the rpiece processes\nin a different way, using some of the openmosix userspace tools.\nNow I use \"runhome oconv [options]\" to compile the octree on the fastest\nmachine, and I launch a first rpiece process that stays on the first\nmachine (\"runhome rpiece @args &\") and several new rpiece processes that\nare able to migrate to the other fast nodes with \"mosrun -t 1 -d 700 rpiece \n@args &\". I understood that it is better to turn off the slower pc's or to\nkeep them in a different network segment, otherwise they will make the\nsystem slower and slower. So a symmetrical cluster is far better.\n\n\nTo make it possible to compare my results, I've benchmarked my 2-node cluster\nwith the bench2 package as taken from Mark Stock's wonderful site, and here \nare the results (radiance was compiled with PIV optimisations).\n\n\nstandard bench2 on the 2.0GHz PIV\n21.92user 0.85system 0:23.45elapsed 97%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (60294major+31417minor)pagefaults 0swaps\nrpict: 0 rays, 0.00% after 0.004u 0.000s 0.004r hours on leviathan\nrpict: 2740602 rays, 32.40% after 0.012u 0.000s 0.013r hours on leviathan\nrpict: 5594096 rays, 42.60% after 0.020u 0.000s 0.021r hours on leviathan\nrpict: 8454012 rays, 48.60% after 0.028u 0.000s 0.029r hours on leviathan\nrpict: 11120053 rays, 52.20% after 0.036u 0.000s 0.038r hours on leviathan\nrpict: 13856057 rays, 56.10% after 0.045u 0.000s 0.047r hours on leviathan\nrpict: 16533987 rays, 60.60% after 0.054u 0.001s 0.055r hours on leviathan\nrpict: 19224383 rays, 66.00% after 0.062u 0.001s 0.064r hours on leviathan\nrpict: 21936654 rays, 71.10% after 0.071u 0.001s 0.072r hours on leviathan\nrpict: 24532107 rays, 75.90% after 0.079u 0.001s 0.081r hours on leviathan\nrpict: 27207234 rays, 81.30% after 0.087u 0.001s 0.089r hours on leviathan\nrpict: 29704006 rays, 100.00% after 0.095u 0.001s 0.097r hours on leviathan\n341.47user 2.70system 5:49.37elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (60349major+31607minor)pagefaults 0swaps\n0.46user 0.00system 0:00.52elapsed 87%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (484major+66minor)pagefaults 0swaps\n\n\nSo the rpict process has ended after 5:49.\n\n\nBy changing the doit script to this:\n\n\nrm -f scene.oct scene.pic scene.amb syncfile args\n/usr/bin/time runhome oconv scene.rad > scene.oct\necho 4 4 > syncfile\necho \"-F syncfile -t 30 -vf v1.vf -af scene.amb -ab 1 -x 1000 -y 1000 \\\n -aa 0.2  -ar 32 -ad 128 -as 0 -lr 6 -lw 0.005 -o scene.pic scene.oct\" > args\n/usr/bin/time runhome rpiece @args &\n/usr/bin/time mosrun -t 1 -d 700 rpiece @args &\n\n\n\n\nI got the following result:\n\n\n21.85user 0.91system 0:22.94elapsed 99%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (60294major+31417minor)pagefaults 0swaps\nFRAME 1: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 1.5 -vl 1.5\nrpict: 0 rays, 0.00% after 0.004u 0.000s 0.008r hours on leviathan\nrpict: 3987 rays, 100.00% after 0.004u 0.000s 0.008r hours on leviathan\nFRAME 2: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 1.5 -vl -0.5\nrpict: 3987 rays, 0.00% after 0.004u 0.000s 0.008r hours on leviathan\nFRAME 1: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 1.5 -vl 0.5\nrpict: 0 rays, 0.00% after 0.004u 0.000s 0.009r hours on leviathan\nrpict: 4154 rays, 100.00% after 0.004u 0.000s 0.009r hours on leviathan\nFRAME 2: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 1.5 -vl -1.5\nrpict: 4154 rays, 0.00% after 0.004u 0.000s 0.009r hours on leviathan\nrpict: 726110 rays, 100.00% after 0.006u 0.000s 0.012r hours on leviathan\nFRAME 3: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 0.5 -vl 1.5\nrpict: 726110 rays, 0.00% after 0.006u 0.000s 0.012r hours on leviathan\nrpict: 492809 rays, 100.00% after 0.005u 0.001s 0.011r hours on leviathan\nFRAME 3: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 0.5 -vl 0.5\nrpict: 492809 rays, 0.00% after 0.005u 0.001s 0.011r hours on leviathan\nrpict: 1324193 rays, 100.00% after 0.008u 0.001s 0.014r hours on leviathan\nFRAME 4: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 0.5 -vl -0.5\nrpict: 1324193 rays, 0.00% after 0.008u 0.001s 0.014r hours on leviathan\nrpict: 3678661 rays, 88.80% after 0.013u 0.001s 0.019r hours on leviathan\nrpict: 3902848 rays, 100.00% after 0.014u 0.001s 0.020r hours on leviathan\nFRAME 4: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs 0.5 -vl -1.5\nrpict: 3902848 rays, 0.00% after 0.014u 0.001s 0.020r hours on leviathan\nrpict: 3959908 rays, 39.60% after 0.016u 0.001s 0.022r hours on leviathan\nrpict: 5912685 rays, 100.00% after 0.022u 0.001s 0.029r hours on leviathan\nFRAME 5: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -0.5 -vl 1.5\nrpict: 5912685 rays, 0.00% after 0.022u 0.001s 0.029r hours on leviathan\nrpict: 6612613 rays, 100.00% after 0.021u 0.001s 0.028r hours on leviathan\nFRAME 5: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -0.5 -vl 0.5\nrpict: 6612613 rays, 0.00% after 0.021u 0.001s 0.028r hours on leviathan\nrpict: 6599351 rays, 100.00% after 0.024u 0.001s 0.031r hours on leviathan\nFRAME 6: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -0.5 -vl -0.5\nrpict: 6599351 rays, 0.00% after 0.024u 0.001s 0.031r hours on leviathan\nrpict: 8845669 rays, 69.60% after 0.027u 0.001s 0.037r hours on leviathan\nrpict: 8907044 rays, 18.00% after 0.031u 0.001s 0.039r hours on leviathan\nrpict: 10518921 rays, 100.00% after 0.031u 0.001s 0.044r hours on leviathan\nFRAME 6: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -0.5 -vl -1.5\nrpict: 10518921 rays, 0.00% after 0.031u 0.001s 0.045r hours on leviathan\nrpict: 11029597 rays, 40.80% after 0.038u 0.001s 0.048r hours on leviathan\nrpict: 13086055 rays, 51.60% after 0.038u 0.001s 0.053r hours on leviathan\nrpict: 13229033 rays, 100.00% after 0.039u 0.001s 0.053r hours on leviathan\nFRAME 7: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -1.5 -vl 1.5\nrpict: 13229033 rays, 0.00% after 0.039u 0.001s 0.053r hours on leviathan\nrpict: 13233114 rays, 100.00% after 0.039u 0.001s 0.053r hours on leviathan\nFRAME 8: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -1.5 -vl 0.5\nrpict: 13233114 rays, 0.00% after 0.039u 0.001s 0.053r hours on leviathan\nrpict: 13263283 rays, 100.00% after 0.039u 0.001s 0.054r hours on leviathan\nFRAME 9: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -1.5 -vl -0.5\nrpict: 13263283 rays, 0.00% after 0.039u 0.001s 0.054r hours on leviathan\nrpict: 13306793 rays, 75.60% after 0.046u 0.001s 0.056r hours on leviathan\nrpict: 14642671 rays, 100.00% after 0.042u 0.001s 0.057r hours on leviathan\nFRAME 10: -vtv -vp 23.6453 -93.6881 26.4175 -vd -0.227921 0.911685 -0.341882 \n-vu 0 0 1 -vh 16.4264 -vv 16.4264 -vo 0 -va 0 -vs -1.5 -vl -1.5\nrpict: 14642671 rays, 0.00% after 0.042u 0.001s 0.057r hours on leviathan\nrpict: 14338103 rays, 100.00% after 0.049u 0.001s 0.059r hours on leviathan\n177.45user 3.49system 3:33.27elapsed 84%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (60493major+32009minor)pagefaults 0swaps\nrpict: 15387552 rays, 100.00% after 0.044u 0.001s 0.059r hours on leviathan\n159.67user 2.77system 3:40.42elapsed 73%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (60894major+33670minor)pagefaults 0swaps\n\n\nSo the rpiece processes ended after 3:40 (~63% of the previous time) \n-> very promising, isn't it?\n\n\nYou may have noticed that I've also shared the ambient file between the\ntwo processes (I haven't cheated, it was erased at the beginning of\nthe simulation), and since the file was accessed on the same computer\nthere was no need to set up a network file system, and there was no need to \nthink about file permissions and so on ...\n\n\nHope you may find this useful ...\n\n\nSorry for not writing very frequently.\nAnd for writing this much this time (!).\n\n\nI hope to see you all at the 2004 workshop\n(any news?).\n\n\n\n\nFrancesco Anselmo\npisuke at blueyonder.co.uk\n___\n<sup>Automatically generated content from [radiance mailing-list](https://radiance-online.org/pipermail/radiance-general/2004-August/001992.html).</sup>", "id": "radiance-general_001992", "created_by": "Francesco_Anselmo"}